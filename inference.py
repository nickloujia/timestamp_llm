import json
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from config import Config
import time
from datetime import datetime
import os

class TimestampInference:
    """æ—¶é—´æˆ³è½¬æ¢æ¨ç†å™¨"""
    
    def __init__(self, model_path=Config.SAVE_PATH):
        """åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        # ä¼˜å…ˆä½¿ç”¨CUDAï¼Œå…¶æ¬¡æ˜¯MPSï¼ˆMacBook GPUï¼‰ï¼Œæœ€åæ˜¯CPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self.load_model()
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        try:
            print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=Config.TRUST_REMOTE_CODE,
                pad_token=Config.PAD_TOKEN
            )
            
            # ç¡®ä¿pad_tokenè®¾ç½®æ­£ç¡®
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # åŠ è½½æ¨¡å‹
            torch_dtype = torch.float32 if Config.TORCH_DTYPE == "float32" else torch.float16
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=torch_dtype,
                device_map=Config.DEVICE_MAP,
                trust_remote_code=Config.TRUST_REMOTE_CODE
            )
            
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”å·²ç»å®Œæˆè®­ç»ƒ")
            raise
    
    def extract_timestamp_from_response(self, response):
        """ä»æ¨¡å‹å›ç­”ä¸­æå–æ—¶é—´æˆ³
        
        Args:
            response: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
            
        Returns:
            str: æå–çš„æ—¶é—´æˆ³ï¼Œæ ¼å¼ä¸º "YYYYå¹´MMæœˆDDæ—¥ HHæ—¶MMåˆ†SSç§’"
        """
        pattern = Config.BEIJING_TIME_PATTERN
        match = re.search(pattern, response)
        if match:
            return match.group(1)
        return None
    
    def generate_response(self, prompt):
        """ç”Ÿæˆæ¨¡å‹å›ç­”
        
        Args:
            prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
            
        Returns:
            str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
        """
        messages = [{"role": "user", "content": prompt}]
        
        try:
            # ä½¿ç”¨chat templateæ ¼å¼åŒ–è¾“å…¥
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            # å¦‚æœchat templateå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
            text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        # tokenizeè¾“å…¥
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS,
                temperature=Config.TEMPERATURE,
                top_k=Config.TOP_K,
                top_p=Config.TOP_P,
                do_sample=Config.DO_SAMPLE,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        return response.strip()
    
    def load_test_data(self, test_file=Config.TEST_FILE):
        """åŠ è½½æµ‹è¯•æ•°æ®
        
        Args:
            test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: æµ‹è¯•æ•°æ®åˆ—è¡¨
        """
        test_data = []
        
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line.strip())
                    test_data.append(item)
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
            return test_data
            
        except FileNotFoundError:
            print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            return []
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return []
    
    def extract_expected_answer(self, messages):
        """ä»æµ‹è¯•æ•°æ®ä¸­æå–æœŸæœ›ç­”æ¡ˆ
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            str: æœŸæœ›çš„æ—¶é—´æˆ³ç­”æ¡ˆ
        """
        # å‡è®¾assistantçš„å›ç­”åŒ…å«æ­£ç¡®çš„æ—¶é—´æˆ³
        for message in messages:
            if message['role'] == 'assistant':
                return self.extract_timestamp_from_response(message['content'])
        return None
    
    def evaluate_single_sample(self, sample, verbose=False):
        """è¯„ä¼°å•ä¸ªæ ·æœ¬
        
        Args:
            sample: å•ä¸ªæµ‹è¯•æ ·æœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        messages = sample['messages']
        user_prompt = messages[0]['content']  # ç”¨æˆ·é—®é¢˜
        expected_answer = self.extract_expected_answer(messages)  # æœŸæœ›ç­”æ¡ˆ
        
        try:
            # ç”Ÿæˆæ¨¡å‹å›ç­”
            start_time = time.time()
            model_response = self.generate_response(user_prompt)
            inference_time = time.time() - start_time
            
            # æå–æ¨¡å‹é¢„æµ‹çš„æ—¶é—´æˆ³
            predicted_answer = self.extract_timestamp_from_response(model_response)
            
            # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
            is_correct = predicted_answer == expected_answer
            
            result = {
                'prompt': user_prompt,
                'expected': expected_answer,
                'predicted': predicted_answer,
                'model_response': model_response,
                'is_correct': is_correct,
                'inference_time': inference_time
            }
            
            if verbose:
                status = "âœ…" if is_correct else "âŒ"
                print(f"{status} é—®é¢˜: {user_prompt[:50]}...")
                print(f"   æœŸæœ›: {expected_answer}")
                print(f"   é¢„æµ‹: {predicted_answer}")
                print(f"   è€—æ—¶: {inference_time:.3f}ç§’")
                if not is_correct:
                    print(f"   å®Œæ•´å›ç­”: {model_response}")
                print()
            
            return result
            
        except Exception as e:
            print(f"âŒ è¯„ä¼°æ ·æœ¬æ—¶å‡ºé”™: {e}")
            return {
                'prompt': user_prompt,
                'expected': expected_answer,
                'predicted': None,
                'model_response': f"Error: {e}",
                'is_correct': False,
                'inference_time': 0
            }
    
    def evaluate_test_set(self, test_file=Config.TEST_FILE, max_samples=None, verbose=True):
        """è¯„ä¼°æ•´ä¸ªæµ‹è¯•é›†
        
        Args:
            test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§æµ‹è¯•æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºæµ‹è¯•æ‰€æœ‰æ ·æœ¬
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            dict: è¯„ä¼°ç»“æœç»Ÿè®¡
        """
        print("="*60)
        print("ğŸ§ª å¼€å§‹æµ‹è¯•é›†è¯„ä¼°")
        print("="*60)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_data(test_file)
        if not test_data:
            return None
        
        # é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°
        if max_samples:
            test_data = test_data[:max_samples]
            print(f"ğŸ“Š é™åˆ¶æµ‹è¯•æ ·æœ¬æ•°: {max_samples}")
        
        print(f"ğŸ“Š æ€»æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
        print()
        
        # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        results = []
        correct_count = 0
        total_inference_time = 0
        
        for i, sample in enumerate(test_data):
            if verbose:
                print(f"[{i+1}/{len(test_data)}]", end=" ")
            
            result = self.evaluate_single_sample(sample, verbose=verbose)
            results.append(result)
            
            if result['is_correct']:
                correct_count += 1
            
            total_inference_time += result['inference_time']
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        accuracy = correct_count / len(test_data) * 100
        avg_inference_time = total_inference_time / len(test_data)
        
        # æ‰“å°ç»“æœç»Ÿè®¡
        print("="*60)
        print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
        print("="*60)
        print(f"âœ… æ­£ç¡®æ ·æœ¬æ•°: {correct_count}")
        print(f"âŒ é”™è¯¯æ ·æœ¬æ•°: {len(test_data) - correct_count}")
        print(f"ğŸ“ˆ å‡†ç¡®ç‡: {accuracy:.2f}% ({correct_count}/{len(test_data)})")
        print(f"â±ï¸  å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.3f}ç§’/æ ·æœ¬")
        print(f"â±ï¸  æ€»æ¨ç†æ—¶é—´: {total_inference_time:.2f}ç§’")
        print("="*60)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self.save_evaluation_results(results, accuracy)
        
        return {
            'total_samples': len(test_data),
            'correct_samples': correct_count,
            'accuracy': accuracy,
            'avg_inference_time': avg_inference_time,
            'total_inference_time': total_inference_time,
            'results': results
        }
    
    def save_evaluation_results(self, results, accuracy):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            results: è¯¦ç»†è¯„ä¼°ç»“æœåˆ—è¡¨
            accuracy: æ€»ä½“å‡†ç¡®ç‡
        """
        try:
            # åˆ›å»ºç»“æœç›®å½•
            results_dir = "./evaluation_results"
            os.makedirs(results_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
            results_file = os.path.join(results_dir, f"evaluation_results_{timestamp}.json")
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': timestamp,
                    'model_path': self.model_path,
                    'accuracy': accuracy,
                    'total_samples': len(results),
                    'correct_samples': sum(1 for r in results if r['is_correct']),
                    'results': results
                }, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜é”™è¯¯æ ·æœ¬åˆ†æ
            error_samples = [r for r in results if not r['is_correct']]
            if error_samples:
                error_file = os.path.join(results_dir, f"error_analysis_{timestamp}.txt")
                with open(error_file, 'w', encoding='utf-8') as f:
                    f.write(f"é”™è¯¯æ ·æœ¬åˆ†æ - {timestamp}\n")
                    f.write(f"æ€»æ ·æœ¬æ•°: {len(results)}\n")
                    f.write(f"é”™è¯¯æ ·æœ¬æ•°: {len(error_samples)}\n")
                    f.write(f"å‡†ç¡®ç‡: {accuracy:.2f}%\n")
                    f.write("="*60 + "\n\n")
                    
                    for i, error in enumerate(error_samples):
                        f.write(f"é”™è¯¯æ ·æœ¬ {i+1}:\n")
                        f.write(f"é—®é¢˜: {error['prompt']}\n")
                        f.write(f"æœŸæœ›ç­”æ¡ˆ: {error['expected']}\n")
                        f.write(f"æ¨¡å‹é¢„æµ‹: {error['predicted']}\n")
                        f.write(f"å®Œæ•´å›ç­”: {error['model_response']}\n")
                        f.write("-" * 40 + "\n\n")
            
            print(f"ğŸ“‹ è¯„ä¼°ç»“æœå·²ä¿å­˜:")
            print(f"   è¯¦ç»†ç»“æœ: {results_file}")
            if error_samples:
                print(f"   é”™è¯¯åˆ†æ: {error_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¯„ä¼°ç»“æœæ—¶å‡ºé”™: {e}")
    
    def interactive_test(self):
        """äº¤äº’å¼æµ‹è¯•æ¨¡å¼"""
        print("="*60)
        print("ğŸ¯ äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("="*60)
        
        while True:
            try:
                user_input = input("\nè¯·è¾“å…¥æ—¶é—´æˆ³è½¬æ¢é—®é¢˜: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ é€€å‡ºäº¤äº’å¼æµ‹è¯•æ¨¡å¼")
                    break
                
                if not user_input:
                    continue
                
                print("ğŸ¤– æ¨¡å‹æ€è€ƒä¸­...")
                start_time = time.time()
                response = self.generate_response(user_input)
                inference_time = time.time() - start_time
                
                print(f"ğŸ“ æ¨¡å‹å›ç­”: {response}")
                
                # å°è¯•æå–æ—¶é—´æˆ³
                timestamp = self.extract_timestamp_from_response(response)
                if timestamp:
                    print(f"ğŸ• æå–çš„æ—¶é—´æˆ³: {timestamp}")
                else:
                    print("âš ï¸ æœªèƒ½ä»å›ç­”ä¸­æå–åˆ°æ ‡å‡†æ ¼å¼çš„æ—¶é—´æˆ³")
                
                print(f"â±ï¸ æ¨ç†æ—¶é—´: {inference_time:.3f}ç§’")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºäº¤äº’å¼æµ‹è¯•æ¨¡å¼")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†è¾“å…¥æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ—¶é—´æˆ³è½¬æ¢æ¨¡å‹æ¨ç†æµ‹è¯•")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(Config.SAVE_PATH):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {Config.SAVE_PATH}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ç”Ÿæˆæ¨¡å‹")
        return
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inference = TimestampInference()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–æ¨ç†å™¨å¤±è´¥: {e}")
        return
    
    # é€‰æ‹©æµ‹è¯•æ¨¡å¼
    print("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
    print("1. æµ‹è¯•é›†è¯„ä¼° (ä½¿ç”¨ data/test.json)")
    print("2. äº¤äº’å¼æµ‹è¯•")
    print("3. ä¸¤ç§æ¨¡å¼éƒ½è¿è¡Œ")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()
    
    if choice in ['1', '3']:
        # æµ‹è¯•é›†è¯„ä¼°
        max_samples = input("è¯·è¾“å…¥æœ€å¤§æµ‹è¯•æ ·æœ¬æ•° (ç›´æ¥å›è½¦è¡¨ç¤ºæµ‹è¯•æ‰€æœ‰æ ·æœ¬): ").strip()
        max_samples = int(max_samples) if max_samples.isdigit() else None
        
        evaluation_results = inference.evaluate_test_set(max_samples=max_samples)
        
        if evaluation_results:
            print(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼æœ€ç»ˆå‡†ç¡®ç‡: {evaluation_results['accuracy']:.2f}%")
    
    if choice in ['2', '3']:
        # äº¤äº’å¼æµ‹è¯•
        inference.interactive_test()

if __name__ == "__main__":
    main()