import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, TrainerCallback
from transformers import DataCollatorForLanguageModeling
import re
import os
from config import Config
import time
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np
import csv

class TimestampDataset(Dataset):
    """æ—¶é—´æˆ³è½¬æ¢æ•°æ®é›† - ç¨³å®šç‰ˆæœ¬"""
    def __init__(self, data_file, tokenizer, max_length=Config.MAX_LENGTH, max_samples=None):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        # åŠ è½½æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                self.data.append(item)
                
                # å¦‚æœè®¾ç½®äº†æœ€å¤§æ ·æœ¬æ•°ï¼Œè¾¾åˆ°é™åˆ¶ååœæ­¢åŠ è½½
                if max_samples and len(self.data) >= max_samples:
                    break
        
        print(f"åŠ è½½äº† {len(self.data)} æ¡è®­ç»ƒæ•°æ®")
        if max_samples:
            print(f"é™åˆ¶æœ€å¤§æ ·æœ¬æ•°: {max_samples}")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        messages = item['messages']
        
        user_msg = messages[0]['content']
        assistant_msg = messages[1]['content']
        
        # åˆ†åˆ«tokenizeç”¨æˆ·æ¶ˆæ¯å’ŒåŠ©æ‰‹æ¶ˆæ¯
        try:
            # æ ¼å¼åŒ–ç”¨æˆ·éƒ¨åˆ†
            user_formatted = self.tokenizer.apply_chat_template(
                [{"role": "user", "content": user_msg}],
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            user_formatted = f"<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n"
        
        # å®Œæ•´çš„å¯¹è¯æ ¼å¼
        try:
            full_formatted = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
        except:
            full_formatted = f"<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant\n{assistant_msg}<|im_end|>"
        
        # tokenizeå®Œæ•´å¯¹è¯
        full_encoding = self.tokenizer(
            full_formatted,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors="pt"
        )
        
        # tokenizeç”¨æˆ·éƒ¨åˆ†ï¼ˆç”¨äºç¡®å®šmaskè¾¹ç•Œï¼‰
        user_encoding = self.tokenizer(
            user_formatted,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors="pt"
        )
        
        input_ids = full_encoding['input_ids'].squeeze()
        user_length = len(user_encoding['input_ids'].squeeze())
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é•¿åº¦
        if len(input_ids) < 2:
            return None
        
        # åˆ›å»ºlabelsï¼Œåªåœ¨assistantå›ç­”éƒ¨åˆ†è®¡ç®—loss
        labels = input_ids.clone()
        # å°†ç”¨æˆ·è¾“å…¥éƒ¨åˆ†çš„labelsè®¾ä¸º-100ï¼ˆä¸è®¡ç®—lossï¼‰
        if user_length < len(labels):
            labels[:user_length] = -100
        
        # è¿›ä¸€æ­¥ä¼˜åŒ–ï¼šåªåœ¨åŒ…å«æ—¶é—´æˆ³ç­”æ¡ˆçš„éƒ¨åˆ†è®¡ç®—loss
        # æŸ¥æ‰¾assistantå›ç­”ä¸­çš„æ—¶é—´æˆ³éƒ¨åˆ†
        assistant_text = assistant_msg
        if "answer:{" in assistant_text:
            # å¦‚æœä½¿ç”¨answer:{}æ ¼å¼ï¼Œé‡ç‚¹è®­ç»ƒè¿™éƒ¨åˆ†
            answer_start = assistant_text.find("answer:{")
            if answer_start > 0:
                # æ‰¾åˆ°answer:{}åœ¨tokenä¸­çš„ä½ç½®
                prefix_text = user_formatted + assistant_text[:answer_start]
                prefix_encoding = self.tokenizer(prefix_text, add_special_tokens=False)
                prefix_length = len(prefix_encoding['input_ids'])
                
                # åªä¿ç•™answeréƒ¨åˆ†çš„lossï¼Œå…¶ä»–assistantæ–‡æœ¬ä¹Ÿmaskæ‰
                if prefix_length < len(labels):
                    labels[user_length:prefix_length] = -100
        
        return {
            'input_ids': input_ids,
            'labels': labels,
            'user_length': user_length  # ç”¨äºè°ƒè¯•
        }

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°"""
    # è¿‡æ»¤æ‰Noneå€¼
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return None
    
    # è·å–æœ€å¤§é•¿åº¦
    max_len = max(len(item['input_ids']) for item in batch)
    
    input_ids = []
    labels = []
    attention_mask = []
    
    for item in batch:
        seq_len = len(item['input_ids'])
        pad_len = max_len - seq_len
        
        # å¡«å……input_ids - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„pad_token_id
        padded_input = torch.cat([
            item['input_ids'],
            torch.full((pad_len,), Config.QWEN_PAD_TOKEN_ID, dtype=torch.long)
        ])
        input_ids.append(padded_input)
        
        # å¡«å……labelsï¼Œpaddingéƒ¨åˆ†è®¾ä¸º-100
        padded_labels = torch.cat([
            item['labels'],
            torch.full((pad_len,), -100, dtype=torch.long)
        ])
        labels.append(padded_labels)
        
        # attention mask
        mask = torch.cat([
            torch.ones(seq_len, dtype=torch.long),
            torch.zeros(pad_len, dtype=torch.long)
        ])
        attention_mask.append(mask)
    
    return {
        'input_ids': torch.stack(input_ids),
        'labels': torch.stack(labels),
        'attention_mask': torch.stack(attention_mask)
    }

class LossLoggingCallback(TrainerCallback):
    """è‡ªå®šä¹‰å›è°ƒå‡½æ•°ï¼Œç”¨äºè¯¦ç»†è®°å½•losså¹¶ä¿å­˜åˆ°æ–‡ä»¶"""
    def __init__(self, save_dir=Config.LOG_DIR):
        self.losses = []
        self.learning_rates = []
        self.steps = []
        self.timestamps = []
        self.accuracies = []  # æ–°å¢ï¼šè®°å½•å‡†ç¡®ç‡
        self.accuracy_steps = []  # æ–°å¢ï¼šè®°å½•å‡†ç¡®ç‡å¯¹åº”çš„æ­¥æ•°
        self.start_time = time.time()
        self.step_times = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        self.csv_file = os.path.join(save_dir, f"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        with open(self.csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['step', 'loss', 'learning_rate', 'elapsed_time', 'timestamp'])
        
        # åˆå§‹åŒ–å‡†ç¡®ç‡CSVæ–‡ä»¶
        self.accuracy_csv_file = os.path.join(save_dir, f"accuracy_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        with open(self.accuracy_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['step', 'accuracy', 'timestamp'])
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and 'loss' in logs:
            current_time = time.time()
            elapsed = current_time - self.start_time
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            # è®°å½•æ•°æ®
            self.losses.append(logs['loss'])
            self.learning_rates.append(logs.get('learning_rate', 0))
            self.steps.append(state.global_step)
            self.timestamps.append(timestamp)
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            with open(self.csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([
                    state.global_step, 
                    logs['loss'], 
                    logs.get('learning_rate', 0), 
                    elapsed, 
                    timestamp
                ])
            
            # è®¡ç®—å¹³å‡lossï¼ˆæœ€è¿‘10æ­¥ï¼‰
            recent_losses = self.losses[-10:]
            avg_loss = sum(recent_losses) / len(recent_losses)
            
            # æ‰“å°è¯¦ç»†ä¿¡æ¯
            print(f"\n[{timestamp}] Step {state.global_step}")
            print(f"  å½“å‰Loss: {logs['loss']:.6f}")
            print(f"  å¹³å‡Loss(æœ€è¿‘10æ­¥): {avg_loss:.6f}")
            print(f"  å­¦ä¹ ç‡: {logs.get('learning_rate', 'N/A')}")
            print(f"  å·²è®­ç»ƒæ—¶é—´: {elapsed/60:.1f}åˆ†é’Ÿ")
            
            # å¦‚æœæœ‰lossä¸‹é™è¶‹åŠ¿ï¼Œæ˜¾ç¤º
            if len(self.losses) >= 20:
                recent_20 = sum(self.losses[-20:-10]) / 10
                current_10 = sum(self.losses[-10:]) / 10
                improvement = recent_20 - current_10
                if improvement > 0:
                    print(f"  âœ… Lossæ”¹å–„: -{improvement:.6f}")
                else:
                    print(f"  âš ï¸  Losså˜åŒ–: {-improvement:.6f}")
            
            # æ ¹æ®é…ç½®æ–‡ä»¶è®¾ç½®çš„æ­¥æ•°ä¿å­˜å›¾è¡¨
            if state.global_step % Config.PLOT_STEPS == 0 and len(self.losses) > 10:
                self.save_loss_plot()
    
    def save_loss_plot(self):
        """ä¿å­˜losså˜åŒ–å›¾è¡¨ï¼ŒåŒ…å«å‡†ç¡®ç‡"""
        try:
            # æ ¹æ®æ˜¯å¦æœ‰å‡†ç¡®ç‡æ•°æ®å†³å®šå­å›¾æ•°é‡
            if len(self.accuracies) > 0:
                fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))
            else:
                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))
            
            # ç»˜åˆ¶lossæ›²çº¿
            ax1.plot(self.steps, self.losses, 'b-', linewidth=1, alpha=0.7, label='Training Loss')
            
            # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
            if len(self.losses) >= 10:
                window_size = min(50, len(self.losses) // 4)
                moving_avg = np.convolve(self.losses, np.ones(window_size)/window_size, mode='valid')
                moving_steps = self.steps[window_size-1:]
                ax1.plot(moving_steps, moving_avg, 'r-', linewidth=2, label=f'Moving Average ({window_size} steps)')
            
            ax1.set_xlabel('Training Steps')
            ax1.set_ylabel('Loss')
            ax1.set_title('Training Loss Over Time')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
            ax2.plot(self.steps, self.learning_rates, 'g-', linewidth=1, label='Learning Rate')
            ax2.set_xlabel('Training Steps')
            ax2.set_ylabel('Learning Rate')
            ax2.set_title('Learning Rate Schedule')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
            
            # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
            if len(self.accuracies) > 0:
                ax3.plot(self.accuracy_steps, self.accuracies, 'purple', marker='o', linewidth=2, markersize=6, label='Test Accuracy')
                ax3.set_xlabel('Training Steps')
                ax3.set_ylabel('Accuracy (%)')
                ax3.set_title('Test Set Accuracy Over Time (100 samples from data/test.json)')
                ax3.legend()
                ax3.grid(True, alpha=0.3)
                ax3.set_ylim(0, 100)  # å‡†ç¡®ç‡èŒƒå›´0-100%
                
                # åœ¨å‡†ç¡®ç‡ç‚¹ä¸Šæ˜¾ç¤ºæ•°å€¼
                for i, (step, acc) in enumerate(zip(self.accuracy_steps, self.accuracies)):
                    ax3.annotate(f'{acc:.1f}%', (step, acc), textcoords="offset points", 
                               xytext=(0,10), ha='center', fontsize=9)
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            plot_file = os.path.join(self.save_dir, f"training_progress_step_{self.steps[-1]}.png")
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"  ğŸ“Š è®­ç»ƒå›¾è¡¨å·²ä¿å­˜: {plot_file}")
            if len(self.accuracies) > 0:
                print(f"  ğŸ“ˆ åŒ…å« {len(self.accuracies)} ä¸ªå‡†ç¡®ç‡æ•°æ®ç‚¹")
            
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜å›¾è¡¨æ—¶å‡ºé”™: {e}")
    
    def save_accuracy_to_csv(self, step, accuracy):
        """ä¿å­˜å‡†ç¡®ç‡åˆ°CSVæ–‡ä»¶"""
        try:
            with open(self.accuracy_csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([step, accuracy, datetime.now().strftime('%Y-%m-%d %H:%M:%S')])
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜å‡†ç¡®ç‡æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def save_final_report(self):
        """ä¿å­˜æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        try:
            report_file = os.path.join(self.save_dir, "training_report.txt")
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write("=== è®­ç»ƒæŠ¥å‘Š ===\n")
                f.write(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.fromtimestamp(self.start_time).strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"è®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ€»è®­ç»ƒæ—¶é—´: {(time.time() - self.start_time)/3600:.2f} å°æ—¶\n")
                f.write(f"æ€»è®­ç»ƒæ­¥æ•°: {len(self.steps)}\n")
                f.write(f"åˆå§‹Loss: {self.losses[0]:.6f}\n")
                f.write(f"æœ€ç»ˆLoss: {self.losses[-1]:.6f}\n")
                f.write(f"Lossæ”¹å–„: {self.losses[0] - self.losses[-1]:.6f}\n")
                f.write(f"æœ€ä½Loss: {min(self.losses):.6f}\n")
                f.write(f"å¹³å‡Loss: {np.mean(self.losses):.6f}\n")
                f.write(f"Lossæ ‡å‡†å·®: {np.std(self.losses):.6f}\n")
            
            print(f"ğŸ“‹ è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è®­ç»ƒæŠ¥å‘Šæ—¶å‡ºé”™: {e}")

class TimestampEvaluator:
    """æ—¶é—´æˆ³è½¬æ¢è¯„ä¼°å™¨"""
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.device = model.device if hasattr(model, 'device') else next(model.parameters()).device
        self.test_data = self.load_test_data()
    
    def load_test_data(self):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        try:
            test_data = []
            with open(Config.TEST_FILE, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line.strip())
                    test_data.append(item)
            print(f"âœ… åŠ è½½äº† {len(test_data)} æ¡æµ‹è¯•æ•°æ®")
            return test_data
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return []
    
    def extract_timestamp_from_response(self, response):
        """ä»æ¨¡å‹å›ç­”ä¸­æå–æ—¶é—´æˆ³"""
        pattern = Config.BEIJING_TIME_PATTERN
        match = re.search(pattern, response)
        if match:
            return match.group(1)
        return None
    
    def extract_expected_answer(self, messages):
        """ä»æµ‹è¯•æ•°æ®ä¸­æå–æœŸæœ›ç­”æ¡ˆ"""
        for message in messages:
            if message['role'] == 'assistant':
                return self.extract_timestamp_from_response(message['content'])
        return None
    
    def generate_response(self, prompt):
        """ç”Ÿæˆæ¨¡å‹å›ç­”"""
        messages = [{"role": "user", "content": prompt}]
        
        try:
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS,
                temperature=Config.TEMPERATURE,
                top_k=Config.TOP_K,
                top_p=Config.TOP_P,
                do_sample=Config.DO_SAMPLE,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    
    def evaluate_test_samples(self, num_samples=100):
        """ä½¿ç”¨æµ‹è¯•é›†è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.test_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„æµ‹è¯•æ•°æ®")
            return 0.0
        
        # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°
        eval_samples = self.test_data[:min(num_samples, len(self.test_data))]
        correct = 0
        total = len(eval_samples)
        
        print(f"\n=== æµ‹è¯•é›†è¯„ä¼° ({total}ä¸ªæ ·æœ¬) ===")
        
        for i, sample in enumerate(eval_samples):
            try:
                messages = sample['messages']
                user_prompt = messages[0]['content']
                expected_answer = self.extract_expected_answer(messages)
                
                # ç”Ÿæˆæ¨¡å‹å›ç­”
                response = self.generate_response(user_prompt)
                predicted_answer = self.extract_timestamp_from_response(response)
                
                is_correct = predicted_answer == expected_answer
                if is_correct:
                    correct += 1
                
                # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 10 == 0 or i == total - 1:
                    print(f"  è¿›åº¦: {i+1}/{total}, å½“å‰å‡†ç¡®ç‡: {correct/(i+1)*100:.1f}%")
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬{i+1}: è¯„ä¼°å‡ºé”™ - {e}")
        
        accuracy = correct / total * 100
        print(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.1f}% ({correct}/{total})")
        return accuracy

class EvaluationCallback(TrainerCallback):
    """è¯„ä¼°å›è°ƒå‡½æ•°"""
    def __init__(self, model, tokenizer, loss_callback, eval_steps=Config.EVAL_STEPS):
        self.evaluator = TimestampEvaluator(model, tokenizer)
        self.eval_steps = eval_steps
        self.last_eval_step = 0
        self.loss_callback = loss_callback  # å¼•ç”¨losså›è°ƒä»¥è®°å½•å‡†ç¡®ç‡
    
    def on_step_end(self, args, state, control, **kwargs):
        if state.global_step - self.last_eval_step >= self.eval_steps:
            print(f"\n{'='*50}")
            print(f"ç¬¬ {state.global_step} æ­¥è¯„ä¼°")
            accuracy = self.evaluator.evaluate_test_samples(num_samples=100)
            
            # å°†å‡†ç¡®ç‡è®°å½•åˆ°losså›è°ƒä¸­
            self.loss_callback.accuracies.append(accuracy)
            self.loss_callback.accuracy_steps.append(state.global_step)
            
            # ä¿å­˜å‡†ç¡®ç‡åˆ°CSVæ–‡ä»¶
            self.loss_callback.save_accuracy_to_csv(state.global_step, accuracy)
            
            print(f"{'='*50}")
            self.last_eval_step = state.global_step

class StableTrainer:
    """ç¨³å®šçš„SFTè®­ç»ƒå™¨"""
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹å’Œtokenizer - ä½¿ç”¨é…ç½®æ–‡ä»¶å‚æ•°
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {Config.MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            Config.MODEL_NAME, 
            trust_remote_code=Config.TRUST_REMOTE_CODE,
            pad_token=Config.PAD_TOKEN
        )
        
        # ç¡®ä¿pad_tokenè®¾ç½®æ­£ç¡®
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        torch_dtype = torch.float32 if Config.TORCH_DTYPE == "float32" else torch.float16
        self.model = AutoModelForCausalLM.from_pretrained(
            Config.MODEL_NAME,
            torch_dtype=torch_dtype,
            device_map=Config.DEVICE_MAP,
            trust_remote_code=Config.TRUST_REMOTE_CODE
        )
        
        # ç¡®ä¿æ¨¡å‹çš„pad_token_idæ­£ç¡®è®¾ç½®
        self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
    
    def train(self):
        """ä½¿ç”¨Transformers Trainerè¿›è¡Œè®­ç»ƒ"""
        print("å¼€å§‹SFTè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€å¤§æ ·æœ¬æ•°
        train_dataset = TimestampDataset(Config.TRAIN_FILE, self.tokenizer, max_samples=Config.MAX_SAMPLES)
        
        # è®­ç»ƒå‚æ•° - ä»é…ç½®æ–‡ä»¶è¯»å–æ‰€æœ‰å‚æ•°
        training_args = TrainingArguments(
            output_dir=Config.SAVE_PATH,
            num_train_epochs=Config.NUM_EPOCHS,
            per_device_train_batch_size=Config.PER_DEVICE_TRAIN_BATCH_SIZE,
            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,
            learning_rate=Config.LEARNING_RATE,
            weight_decay=Config.WEIGHT_DECAY,
            warmup_steps=Config.WARMUP_STEPS,
            logging_steps=Config.LOGGING_STEPS,
            save_steps=Config.SAVE_STEPS,
            save_strategy=Config.SAVE_STRATEGY,
            fp16=Config.FP16,
            max_grad_norm=Config.MAX_GRAD_NORM,
        )
        
        # åˆ›å»ºå›è°ƒå‡½æ•°
        loss_callback = LossLoggingCallback(save_dir=Config.LOG_DIR)
        eval_callback = EvaluationCallback(self.model, self.tokenizer, loss_callback, eval_steps=Config.EVAL_STEPS)
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            data_collator=collate_fn,
            callbacks=[loss_callback, eval_callback],
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹è®­ç»ƒ...")
        try:
            trainer.train()
            print("è®­ç»ƒå®Œæˆï¼")
            
            # ä¿å­˜æœ€ç»ˆçš„losså›¾è¡¨å’ŒæŠ¥å‘Š
            loss_callback.save_loss_plot()
            loss_callback.save_final_report()
            
            # ä¿å­˜æ¨¡å‹
            trainer.save_model(Config.SAVE_PATH)
            self.tokenizer.save_pretrained(Config.SAVE_PATH)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {Config.SAVE_PATH}")
            
            # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
            self.print_training_summary(loss_callback)
            
        except Exception as e:
            print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def print_training_summary(self, loss_callback):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        if len(loss_callback.losses) > 0:
            print("\n" + "="*60)
            print("ğŸ‰ è®­ç»ƒæ€»ç»“")
            print("="*60)
            print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {len(loss_callback.steps)}")
            print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {(time.time() - loss_callback.start_time)/3600:.2f} å°æ—¶")
            print(f"ğŸ“‰ åˆå§‹Loss: {loss_callback.losses[0]:.6f}")
            print(f"ğŸ“‰ æœ€ç»ˆLoss: {loss_callback.losses[-1]:.6f}")
            print(f"ğŸ“ˆ Lossæ”¹å–„: {loss_callback.losses[0] - loss_callback.losses[-1]:.6f}")
            print(f"ğŸ¯ æœ€ä½Loss: {min(loss_callback.losses):.6f}")
            print(f"ğŸ“‹ è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {loss_callback.save_dir}")
            print("="*60)

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½å¹¶æ˜¾ç¤ºè®­ç»ƒç›®æ ‡"""
    print("æµ‹è¯•æ•°æ®åŠ è½½...")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=Config.TRUST_REMOTE_CODE)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æœ€å¤§æ ·æœ¬æ•°
    dataset = TimestampDataset(Config.TRAIN_FILE, tokenizer, max_samples=Config.MAX_SAMPLES)
    
    # è¯¦ç»†åˆ†æå‰å‡ ä¸ªæ ·æœ¬çš„è®­ç»ƒç›®æ ‡
    print("\n" + "="*60)
    print("ğŸ“Š è®­ç»ƒç›®æ ‡åˆ†æ (æ˜¾ç¤ºæ¨¡å‹å®é™…å­¦ä¹ çš„å†…å®¹)")
    print("="*60)
    
    for i in range(min(3, len(dataset))):
        item = dataset[i]
        if item is not None:
            print(f"\næ ·æœ¬ {i+1}:")
            print("-" * 40)
            
            input_ids = item['input_ids']
            labels = item['labels']
            user_length = item.get('user_length', 0)
            
            # è§£ç å®Œæ•´è¾“å…¥
            full_text = tokenizer.decode(input_ids, skip_special_tokens=True)
            print(f"å®Œæ•´å¯¹è¯:\n{full_text}")
            
            # æ˜¾ç¤ºå“ªäº›éƒ¨åˆ†ä¼šè¢«è®­ç»ƒï¼ˆlabels != -100ï¼‰
            training_tokens = []
            training_text_parts = []
            
            for j, (token_id, label_id) in enumerate(zip(input_ids, labels)):
                if label_id != -100:  # è¿™äº›tokenä¼šè¢«è®­ç»ƒ
                    training_tokens.append(token_id.item())
                    training_text_parts.append(tokenizer.decode([token_id], skip_special_tokens=True))
            
            if training_tokens:
                training_text = tokenizer.decode(training_tokens, skip_special_tokens=True)
                print(f"\nğŸ¯ æ¨¡å‹å®é™…å­¦ä¹ çš„å†…å®¹:")
                print(f"'{training_text}'")
                print(f"ğŸ“Š è®­ç»ƒtokenæ•°: {len(training_tokens)} / {len(input_ids)} (æ€»æ•°)")
                print(f"ğŸ“Š è®­ç»ƒæ¯”ä¾‹: {len(training_tokens)/len(input_ids)*100:.1f}%")
            else:
                print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒç›®æ ‡token!")
            
            # æ˜¾ç¤ºè¢«maskçš„éƒ¨åˆ†
            masked_tokens = []
            for j, (token_id, label_id) in enumerate(zip(input_ids, labels)):
                if label_id == -100:
                    masked_tokens.append(token_id.item())
            
            if masked_tokens:
                masked_text = tokenizer.decode(masked_tokens, skip_special_tokens=True)
                print(f"ğŸš« è¢«å¿½ç•¥çš„å†…å®¹ (ä¸å‚ä¸è®­ç»ƒ):")
                print(f"'{masked_text}'")
    
    print("\n" + "="*60)
    
    # æµ‹è¯•æ‰¹å¤„ç†
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)
    batch = next(iter(dataloader))
    if batch is not None:
        print("æ‰¹å¤„ç†æµ‹è¯•:")
        print(f"  input_ids shape: {batch['input_ids'].shape}")
        print(f"  labels shape: {batch['labels'].shape}")
        print(f"  attention_mask shape: {batch['attention_mask'].shape}")
        
        # ç»Ÿè®¡è®­ç»ƒtokenæ¯”ä¾‹
        total_tokens = batch['labels'].numel()
        training_tokens = (batch['labels'] != -100).sum().item()
        print(f"  è®­ç»ƒtokenæ¯”ä¾‹: {training_tokens/total_tokens*100:.1f}%")
    
    print("æ•°æ®åŠ è½½æµ‹è¯•å®Œæˆï¼")

def main():
    """ä¸»å‡½æ•°"""
    print("=== ç¨³å®šç‰ˆæœ¬ Qwen SFTè®­ç»ƒ ===")
    print(f"é…ç½®ä¿¡æ¯:")
    print(f"  æ¨¡å‹: {Config.MODEL_NAME}")
    print(f"  æœ€å¤§æ ·æœ¬æ•°: {Config.MAX_SAMPLES}")
    print(f"  è®­ç»ƒè½®æ•°: {Config.NUM_EPOCHS}")
    print(f"  æ‰¹æ¬¡å¤§å°: {Config.PER_DEVICE_TRAIN_BATCH_SIZE}")
    print(f"  å­¦ä¹ ç‡: {Config.LEARNING_RATE}")
    
    # é¦–å…ˆæµ‹è¯•æ•°æ®åŠ è½½
    test_data_loading()
    
    # å¼€å§‹è®­ç»ƒ
    trainer = StableTrainer()
    trainer.train()

if __name__ == "__main__":
    main()