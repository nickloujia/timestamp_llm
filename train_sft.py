import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import re
import os
from config import Config

class TimestampDataset(Dataset):
    """æ—¶é—´æˆ³è½¬æ¢æ•°æ®é›†"""
    def __init__(self, data_file, tokenizer, max_length=Config.MAX_LENGTH):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        # åŠ è½½æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                self.data.append(json.loads(line.strip()))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        messages = item['messages']
        
        # ä½¿ç”¨ç®€åŒ–çš„æ–¹æ³•ï¼šç›´æ¥æ ¼å¼åŒ–å®Œæ•´å¯¹è¯
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=False
        )
        
        # tokenize
        tokens = self.tokenizer.encode(formatted_text, max_length=self.max_length, truncation=True)
        
        # ç¡®ä¿åºåˆ—é•¿åº¦è‡³å°‘ä¸º2
        if len(tokens) < 2:
            tokens = tokens + [self.tokenizer.eos_token_id]
        
        # åˆ›å»ºè¾“å…¥å’Œæ ‡ç­¾
        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)
        labels = torch.tensor(tokens[1:], dtype=torch.long)
        
        return {
            'input_ids': input_ids,
            'target_ids': labels,
            'attention_mask': torch.ones_like(input_ids)
        }

class QwenSFTTrainer:
    """åŸºäºQwen2.5-1.5Bçš„SFTè®­ç»ƒå™¨"""
    def __init__(self):
        # ä¼˜å…ˆä½¿ç”¨CUDAï¼Œå…¶æ¬¡æ˜¯MPSï¼ˆMacBook GPUï¼‰ï¼Œæœ€åæ˜¯CPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œtokenizer
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {Config.MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=Config.TRUST_REMOTE_CODE)
        
        # æ ¹æ®è®¾å¤‡ç±»å‹è°ƒæ•´åŠ è½½å‚æ•°
        if self.device.type == 'mps':
            # MPSè®¾å¤‡ä½¿ç”¨float32ä»¥æé«˜å…¼å®¹æ€§
            torch_dtype = torch.float32
            device_map = None  # è®©æ¨¡å‹åŠ è½½åˆ°CPUï¼Œç„¶åæ‰‹åŠ¨ç§»åŠ¨åˆ°MPS
        else:
            torch_dtype = getattr(torch, Config.TORCH_DTYPE)
            device_map = Config.DEVICE_MAP
        
        self.model = AutoModelForCausalLM.from_pretrained(
            Config.MODEL_NAME,
            torch_dtype=torch_dtype,
            device_map=device_map,
            trust_remote_code=Config.TRUST_REMOTE_CODE
        )
        
        # å¦‚æœæ˜¯MPSè®¾å¤‡ï¼Œæ‰‹åŠ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°MPS
        if self.device.type == 'mps':
            self.model = self.model.to(self.device)
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ä¼˜åŒ–å™¨ï¼ˆä¸éœ€è¦å•ç‹¬çš„æŸå¤±å‡½æ•°ï¼Œä½¿ç”¨æ¨¡å‹å†…ç½®çš„ï¼‰
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=Config.LEARNING_RATE, 
            weight_decay=Config.WEIGHT_DECAY
        )
        
        print("æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
    
    def quick_evaluate(self, evaluator, num_samples=20):
        """å¿«é€Ÿè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            accuracy, _ = evaluator.evaluate_model(num_samples=num_samples, verbose=False)
            return accuracy
        except Exception as e:
            print(f"å¿«é€Ÿè¯„ä¼°å‡ºé”™: {e}")
            return 0.0

    def train_epoch(self, dataloader, evaluator=None, eval_interval=100):
        """è®­ç»ƒä¸€ä¸ªepochï¼Œåªæ˜¾ç¤ºå…³é”®ä¿¡æ¯"""
        self.model.train()
        total_loss = 0
        num_batches = len(dataloader)
        
        progress_bar = tqdm(dataloader, desc="è®­ç»ƒä¸­")
        
        for batch_idx, batch in enumerate(progress_bar):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch['input_ids'].to(self.device)
            target_ids = batch['target_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            
            # åŸºæœ¬çš„æ•°æ®éªŒè¯
            if torch.isnan(input_ids).any() or torch.isinf(input_ids).any():
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} input_ids åŒ…å«å¼‚å¸¸å€¼")
                continue
            if torch.isnan(target_ids).any() or torch.isinf(target_ids).any():
                print(f"âŒ æ‰¹æ¬¡ {batch_idx} target_ids åŒ…å«å¼‚å¸¸å€¼")
                continue
            
            try:
                # å‰å‘ä¼ æ’­ - ä½¿ç”¨æ¨¡å‹å†…ç½®çš„æŸå¤±è®¡ç®—
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=target_ids  # æ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†-100çš„å¿½ç•¥
                )
                
                loss = outputs.loss
                
                # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦ä¸ºNaNæˆ–Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âŒ æ‰¹æ¬¡ {batch_idx} æŸå¤±å€¼å¼‚å¸¸: {loss.item()}")
                    continue
                
                # åå‘ä¼ æ’­
                loss.backward()
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaNæˆ–Inf
                has_nan_grad = False
                for name, param in self.model.named_parameters():
                    if param.grad is not None:
                        if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():
                            print(f"âŒ æ‰¹æ¬¡ {batch_idx} å‚æ•° {name} æ¢¯åº¦å¼‚å¸¸")
                            has_nan_grad = True
                            break
                
                if has_nan_grad:
                    self.optimizer.zero_grad()
                    continue
                
                # æ¢¯åº¦ç´¯ç§¯
                if (batch_idx + 1) % Config.GRADIENT_ACCUMULATION_STEPS == 0:
                    # æ¢¯åº¦è£å‰ª
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=Config.MAX_GRAD_NORM)
                    
                    # æ£€æŸ¥æ¢¯åº¦èŒƒæ•°
                    if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                        print(f"âŒ æ‰¹æ¬¡ {batch_idx} æ¢¯åº¦èŒƒæ•°å¼‚å¸¸: {grad_norm}")
                        self.optimizer.zero_grad()
                        continue
                    
                    self.optimizer.step()
                    self.optimizer.zero_grad()
                
                total_loss += loss.item()
                
                # æ›´æ–°è¿›åº¦æ¡
                avg_loss = total_loss / (batch_idx + 1)
                progress_bar.set_postfix({'loss': f'{avg_loss:.4f}'})
                
                # å®šæœŸæµ‹è¯•
                if evaluator and (batch_idx + 1) % eval_interval == 0:
                    accuracy = self.quick_evaluate(evaluator, num_samples=20)
                    print(f"\n æ‰¹æ¬¡ {batch_idx + 1} - å‡†ç¡®ç‡: {accuracy:.2f}%")
                
            except Exception as e:
                print(f"æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        return total_loss / num_batches
    
    def train(self):
        """è®­ç»ƒæ¨¡å‹"""
        print("å¼€å§‹SFTè®­ç»ƒ...")
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = TimestampDataset(Config.TRAIN_FILE, self.tokenizer)
        train_loader = DataLoader(
            train_dataset,
            batch_size=Config.BATCH_SIZE,
            shuffle=True,
            collate_fn=self._collate_fn
        )
        
        print(f"è®­ç»ƒæ•°æ®: {len(train_dataset)} æ¡")
        print(f"æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE}")
        print(f"è®­ç»ƒè½®æ•°: {Config.NUM_EPOCHS}")
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(Config.NUM_EPOCHS):
            print(f"\n=== Epoch {epoch + 1}/{Config.NUM_EPOCHS} ===")
            
            avg_loss = self.train_epoch(train_loader)
            print(f"Epoch {epoch + 1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % 1 == 0:
                self.save_model(f"{Config.SAVE_PATH}_epoch_{epoch + 1}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_model(Config.SAVE_PATH)
        print(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {Config.SAVE_PATH}")
    
    def _collate_fn(self, batch):
        """æ•°æ®æ‰¹å¤„ç†å‡½æ•°"""
        # æ‰¾åˆ°æœ€å¤§é•¿åº¦
        max_len = max(len(item['input_ids']) for item in batch)
        
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        input_ids = []
        target_ids = []
        attention_mask = []
        
        for item in batch:
            # å¡«å……input_ids
            pad_len = max_len - len(item['input_ids'])
            input_ids.append(torch.cat([item['input_ids'], torch.full((pad_len,), self.tokenizer.pad_token_id, dtype=torch.long)]))
            # å¡«å……target_ids - ä½¿ç”¨-100å¿½ç•¥paddingéƒ¨åˆ†çš„æŸå¤±
            target_ids.append(torch.cat([item['target_ids'], torch.full((pad_len,), -100, dtype=torch.long)]))
            # åˆ›å»ºattention_mask - paddingéƒ¨åˆ†ä¸º0
            mask = torch.cat([torch.ones(len(item['input_ids']), dtype=torch.long), torch.zeros(pad_len, dtype=torch.long)])
            attention_mask.append(mask)
            
        return {
            'input_ids': torch.stack(input_ids),
            'target_ids': torch.stack(target_ids),
            'attention_mask': torch.stack(attention_mask)
        }
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹å’Œtokenizer
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {path}")

class QwenEvaluator:
    """åŸºäºQwenæ¨¡å‹çš„è¯„ä¼°å™¨"""
    def __init__(self, model_path=Config.SAVE_PATH):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=Config.TRUST_REMOTE_CODE)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=getattr(torch, Config.TORCH_DTYPE),
            device_map=Config.DEVICE_MAP,
            trust_remote_code=Config.TRUST_REMOTE_CODE
        )
        
        print("è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼")
    
    def extract_timestamp_from_response(self, response):
        """ä»æ¨¡å‹å›ç­”ä¸­æå–æ—¶é—´æˆ³"""
        match = re.search(Config.BEIJING_TIME_PATTERN, response)
        if match:
            return match.group(1)
        return None
    
    def generate_response(self, prompt):
        """ç”Ÿæˆå›ç­”"""
        # æ„å»ºå¯¹è¯æ ¼å¼
        messages = [
            {"role": "user", "content": prompt}
        ]
        
        # ä½¿ç”¨tokenizeræ ¼å¼åŒ–è¾“å…¥
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # ç”Ÿæˆå›ç­”
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=Config.MAX_NEW_TOKENS,
                temperature=Config.TEMPERATURE,
                top_k=Config.TOP_K,
                top_p=Config.TOP_P,
                do_sample=Config.DO_SAMPLE,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç å›ç­”
        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    
    def evaluate_model(self, num_samples=50, verbose=True, show_examples=True):
        """è¯„ä¼°æ¨¡å‹ï¼Œæ”¯æŒé™é»˜æ¨¡å¼å’Œæ ·æœ¬å±•ç¤º"""
        if verbose:
            print(f"å¼€å§‹è¯„ä¼°æ¨¡å‹ï¼Œä½¿ç”¨ {num_samples} ä¸ªæµ‹è¯•æ ·æœ¬...")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_data = []
        with open(Config.TEST_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                test_data.append(json.loads(line.strip()))
        
        test_data = test_data[:num_samples]
        
        correct_count = 0
        total_count = len(test_data)
        results = []
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        if show_examples and verbose:
            print(f"\n{'='*60}")
            print("ğŸ“Š æ ·æœ¬åˆ†æ (å‰5ä¸ªæ ·æœ¬)")
            print(f"{'='*60}")
        
        for i, data in enumerate(tqdm(test_data, desc="è¯„ä¼°è¿›åº¦")):
            user_content = data['messages'][0]['content']
            expected_response = data['messages'][1]['content']
            
            # æå–æ—¶é—´æˆ³
            timestamp_match = re.search(Config.TIMESTAMP_PATTERN, user_content)
            if not timestamp_match:
                if show_examples and i < 5:
                    print(f"âŒ æ ·æœ¬ {i+1}: æ— æ³•ä»ç”¨æˆ·è¾“å…¥ä¸­æå–æ—¶é—´æˆ³")
                    print(f"   ç”¨æˆ·è¾“å…¥: {user_content}")
                continue
            
            timestamp = int(timestamp_match.group(1))
            
            try:
                # ç”Ÿæˆå›ç­”
                model_response = self.generate_response(user_content)
                
                # æå–æ¨¡å‹å›ç­”ä¸­çš„æ—¶é—´
                extracted_time = self.extract_timestamp_from_response(model_response)
                
                # è®¡ç®—æ­£ç¡®ç­”æ¡ˆ
                expected_time_match = re.search(Config.BEIJING_TIME_PATTERN, expected_response)
                expected_time = expected_time_match.group(1) if expected_time_match else None
                
                # åˆ¤æ–­æ˜¯å¦æ­£ç¡®
                is_correct = extracted_time == expected_time
                if is_correct:
                    correct_count += 1
                
                results.append({
                    'timestamp': timestamp,
                    'expected': expected_time,
                    'predicted': extracted_time,
                    'model_response': model_response,
                    'is_correct': is_correct
                })
                
                # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                if show_examples and i < 5 and verbose:
                    status = "âœ…" if is_correct else "âŒ"
                    print(f"\n{status} æ ·æœ¬ {i+1}:")
                    print(f"   æ—¶é—´æˆ³: {timestamp}")
                    print(f"   ç”¨æˆ·é—®é¢˜: {user_content}")
                    print(f"   æœŸæœ›ç­”æ¡ˆ: {expected_time}")
                    print(f"   æ¨¡å‹å›ç­”: {model_response}")
                    print(f"   æå–ç»“æœ: {extracted_time}")
                    print(f"   åŒ¹é…ç»“æœ: {'æ­£ç¡®' if is_correct else 'é”™è¯¯'}")
                
                # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 10 == 0:
                    current_accuracy = correct_count / (i + 1) * 100
                    if verbose:
                        print(f"\nğŸ“Š è¿›åº¦æ›´æ–°: {current_accuracy:.2f}% ({correct_count}/{i+1})")
                
            except Exception as e:
                if show_examples and i < 5:
                    print(f"âŒ æ ·æœ¬ {i+1}: å¤„ç†å‡ºé”™ - {e}")
                    print(f"   ç”¨æˆ·è¾“å…¥: {user_content}")
                continue
        
        # è®¡ç®—æœ€ç»ˆå‡†ç¡®ç‡
        final_accuracy = correct_count / total_count * 100
        
        if verbose:
            print(f"\n{'='*60}")
            print("ğŸ“Š æœ€ç»ˆè¯„ä¼°ç»“æœ")
            print(f"{'='*60}")
            print(f"æ€»æ ·æœ¬æ•°: {total_count}")
            print(f"æ­£ç¡®å›ç­”: {correct_count}")
            print(f"å‡†ç¡®ç‡: {final_accuracy:.2f}%")
            
            # æ˜¾ç¤ºä¸€äº›é”™è¯¯æ ·æœ¬
            error_samples = [r for r in results if not r['is_correct']]
            if error_samples and len(error_samples) > 0:
                print(f"\nâŒ é”™è¯¯æ ·æœ¬åˆ†æ (æ˜¾ç¤ºå‰3ä¸ª):")
                for j, error in enumerate(error_samples[:3]):
                    print(f"   é”™è¯¯ {j+1}:")
                    print(f"     æ—¶é—´æˆ³: {error['timestamp']}")
                    print(f"     æœŸæœ›: {error['expected']}")
                    print(f"     é¢„æµ‹: {error['predicted']}")
                    print(f"     æ¨¡å‹å›ç­”: {error['model_response'][:100]}...")
        
        return final_accuracy, results

def check_data_format():
    """æ£€æŸ¥æ•°æ®æ ¼å¼"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒæ•°æ®æ ¼å¼...")
    
    try:
        # æ£€æŸ¥è®­ç»ƒæ•°æ®
        with open(Config.TRAIN_FILE, 'r', encoding='utf-8') as f:
            train_samples = []
            for i, line in enumerate(f):
                if i >= 5:  # åªæ£€æŸ¥å‰5ä¸ªæ ·æœ¬
                    break
                train_samples.append(json.loads(line.strip()))
        
        print(f"\nğŸ“Š è®­ç»ƒæ•°æ®æ ·æœ¬ (å‰5ä¸ª):")
        for i, sample in enumerate(train_samples):
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"  ç”¨æˆ·: {sample['messages'][0]['content']}")
            print(f"  åŠ©æ‰‹: {sample['messages'][1]['content']}")
            
            # æ£€æŸ¥æ—¶é—´æˆ³æå–
            timestamp_match = re.search(Config.TIMESTAMP_PATTERN, sample['messages'][0]['content'])
            if timestamp_match:
                print(f"  âœ… æ—¶é—´æˆ³: {timestamp_match.group(1)}")
            else:
                print(f"  âŒ æ— æ³•æå–æ—¶é—´æˆ³")
            
            # æ£€æŸ¥åŒ—äº¬æ—¶é—´æå–
            beijing_match = re.search(Config.BEIJING_TIME_PATTERN, sample['messages'][1]['content'])
            if beijing_match:
                print(f"  âœ… åŒ—äº¬æ—¶é—´: {beijing_match.group(1)}")
            else:
                print(f"  âŒ æ— æ³•æå–åŒ—äº¬æ—¶é—´")
        
        # æ£€æŸ¥æµ‹è¯•æ•°æ®
        print(f"\nğŸ“Š æµ‹è¯•æ•°æ®æ ·æœ¬ (å‰3ä¸ª):")
        with open(Config.TEST_FILE, 'r', encoding='utf-8') as f:
            test_samples = []
            for i, line in enumerate(f):
                if i >= 3:  # åªæ£€æŸ¥å‰3ä¸ªæ ·æœ¬
                    break
                test_samples.append(json.loads(line.strip()))
        
        for i, sample in enumerate(test_samples):
            print(f"\næµ‹è¯•æ ·æœ¬ {i+1}:")
            print(f"  ç”¨æˆ·: {sample['messages'][0]['content']}")
            print(f"  åŠ©æ‰‹: {sample['messages'][1]['content']}")
            
            # æ£€æŸ¥æ—¶é—´æˆ³æå–
            timestamp_match = re.search(Config.TIMESTAMP_PATTERN, sample['messages'][0]['content'])
            if timestamp_match:
                print(f"  âœ… æ—¶é—´æˆ³: {timestamp_match.group(1)}")
            else:
                print(f"  âŒ æ— æ³•æå–æ—¶é—´æˆ³")
            
            # æ£€æŸ¥åŒ—äº¬æ—¶é—´æå–
            beijing_match = re.search(Config.BEIJING_TIME_PATTERN, sample['messages'][1]['content'])
            if beijing_match:
                print(f"  âœ… åŒ—äº¬æ—¶é—´: {beijing_match.group(1)}")
            else:
                print(f"  âŒ æ— æ³•æå–åŒ—äº¬æ—¶é—´")
        
        print(f"\nâœ… æ•°æ®æ ¼å¼æ£€æŸ¥å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼æ£€æŸ¥å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== Qwen2.5-1.5B Timestampè½¬æ¢SFTè®­ç»ƒ ===")
    
    # é¦–å…ˆæ£€æŸ¥æ•°æ®æ ¼å¼
    check_data_format()
    
    # è¯¢é—®æ˜¯å¦ç»§ç»­
    choice = input("\næ•°æ®æ ¼å¼æ£€æŸ¥å®Œæˆï¼Œæ˜¯å¦ç»§ç»­ï¼Ÿ(y/n): ").lower()
    if choice != 'y':
        return
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹
    if os.path.exists(Config.SAVE_PATH):
        print(f"å‘ç°å·²è®­ç»ƒçš„æ¨¡å‹: {Config.SAVE_PATH}")
        choice = input("æ˜¯å¦ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Ÿ(y/n): ").lower()
        
        if choice == 'y':
            # ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œè¯„ä¼°
            evaluator = QwenEvaluator(Config.SAVE_PATH)
            accuracy, results = evaluator.evaluate_model(num_samples=20, show_examples=True)
            print(f"\nè®­ç»ƒåæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.2f}%")
            return
    
    # è®­ç»ƒæ–°æ¨¡å‹
    print("å¼€å§‹è®­ç»ƒæ–°æ¨¡å‹...")
    trainer = QwenSFTTrainer()
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹
    print("\nå¼€å§‹è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹...")
    evaluator = QwenEvaluator(Config.SAVE_PATH)
    accuracy, results = evaluator.evaluate_model(num_samples=50, show_examples=True)
    
    print(f"\nè®­ç»ƒåæ¨¡å‹å‡†ç¡®ç‡: {accuracy:.2f}%")
    
    if accuracy < 50:
        print("âš ï¸  æ¨¡å‹å‡†ç¡®ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ è®­ç»ƒæ•°æ®æˆ–è°ƒæ•´è®­ç»ƒå‚æ•°")
    elif accuracy < 80:
        print("âš ï¸  æ¨¡å‹å‡†ç¡®ç‡ä¸­ç­‰ï¼Œå¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âœ… æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼")

if __name__ == "__main__":
    main() 